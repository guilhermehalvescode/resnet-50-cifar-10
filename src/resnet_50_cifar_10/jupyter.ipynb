{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard\n",
    "\n",
    "# Data Preparation\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "validation_split = 0.1\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(trainset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(validation_split * dataset_size)\n",
    "\n",
    "if shuffle_dataset:\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "    torch.utils.data.random_split(trainset, [split, dataset_size - split])\n",
    "\n",
    "train_sampler = SubsetRandomSampler(indices[split:])\n",
    "valid_sampler = SubsetRandomSampler(indices[:split])\n",
    "\n",
    "trainloader = data.DataLoader(\n",
    "    trainset, batch_size=128, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "validloader = data.DataLoader(\n",
    "    trainset, batch_size=128, sampler=valid_sampler, num_workers=2\n",
    ")\n",
    "testloader = data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = 5\n",
    "best_valid_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(\"runs/resnet50_cifar10\")  # Create a SummaryWriter\n",
    "\n",
    "# Initialize batch counters\n",
    "train_batch_num = 0\n",
    "valid_batch_num = 0\n",
    "\n",
    "# Training Loop with Early Stopping and Detailed TensorBoard Logging\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Log per-batch metrics for training\n",
    "        train_loss = loss.item()\n",
    "        train_acc = 100.0 * predicted.eq(labels).sum().item() / labels.size(0)\n",
    "        writer.add_scalar(\"Batch_Loss/Train\", train_loss, train_batch_num)\n",
    "        writer.add_scalar(\"Batch_Accuracy/Train\", train_acc, train_batch_num)\n",
    "        train_batch_num += 1\n",
    "\n",
    "    train_loss = running_loss / len(trainloader.sampler)\n",
    "    train_acc = 100.0 * correct / total\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Log per-batch metrics for validation\n",
    "            batch_valid_loss = loss.item()\n",
    "            batch_valid_acc = 100.0 * \\\n",
    "                predicted.eq(labels).sum().item() / labels.size(0)\n",
    "            writer.add_scalar(\n",
    "                \"Batch_Loss/Validation\", batch_valid_loss, valid_batch_num\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Batch_Accuracy/Validation\", batch_valid_acc, valid_batch_num\n",
    "            )\n",
    "            valid_batch_num += 1\n",
    "\n",
    "    valid_loss = valid_loss / len(validloader.sampler)\n",
    "    valid_acc = 100.0 * correct / total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "        f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Log metrics to TensorBoard for each epoch\n",
    "    writer.add_scalar(\"Epoch_Loss/Train\", train_loss, epoch + 1)\n",
    "    writer.add_scalar(\"Epoch_Loss/Validation\", valid_loss, epoch + 1)\n",
    "    writer.add_scalar(\"Epoch_Accuracy/Train\", train_acc, epoch + 1)\n",
    "    writer.add_scalar(\"Epoch_Accuracy/Validation\", valid_acc, epoch + 1)\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save the best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "# Testing Loop\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))  # Load the best model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_acc = 100.0 * correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "testloader = data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Step 6: Testing Loop\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))  # Load the best model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_acc = 100.0 * correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
